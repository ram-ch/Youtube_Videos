{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89892f1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbe383",
   "metadata": {},
   "source": [
    "\n",
    "***RAG-Based Chatbot for Web content interaction*** simplifies data retrieval by scraping data from a given URL and converting it to structured JSON format. Additionally, it also extends to answering user questions based on the extracted data..\n",
    "\n",
    "In this notebook, we will be looking at a sample code for execution of RAG based chatbot for webscraping.\n",
    "\n",
    "This project utilizes Two agents , namely -  \n",
    "\n",
    "**1. Scrapping agent** - which is responsible to scrap data from the URL (provided as input) to save it as a JSON format file named - scrapped_data.json\n",
    "\n",
    "**2. Chatbot with memory Agent** - which takes the information from the JSON format file to create a vector and saves (i.e upserts) the vector embedding in Pinecone (vector database). A chatbot with memory will be used based on which user can question upon the URL link uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec551f2",
   "metadata": {},
   "source": [
    "## Functionality overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87553a",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "Design and implement a set of intelligent agents that work together to \n",
    "\n",
    "(1) scrape content from a specified website URL and save it in a structured format, and \n",
    "\n",
    "(2) utilize the scraped data to answer user queries about the website content.\n",
    "\n",
    "**Input:**\n",
    "- Task 1: URL of the webpage.\n",
    "- Task 2: Any query regarding the scraped data.\n",
    "\n",
    "**Output:**\n",
    "- Task 1: JSON file with scraped data.\n",
    "- Task 2: Answer to questions regarding scrapped web page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3122e",
   "metadata": {},
   "source": [
    "## Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e0f4e",
   "metadata": {},
   "source": [
    " PART 1 : SCRAPPING AGENT\n",
    " \n",
    " \n",
    "    1.1 Scrape data from the webpage (URL - input)  \n",
    "    \n",
    "    1.2 Save the scrapped data in JSON format\n",
    "    \n",
    "    ____________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f9b15",
   "metadata": {},
   "source": [
    "PART 2 : CHATBOT WITH MEMORY AGENT \n",
    "    \n",
    "    \n",
    "    2.1 Retrieve JSON data and format it to string\n",
    "    \n",
    "    2.2 Split the document to chunks (for creating and upserting embeddings)\n",
    "    \n",
    "    2.3 Text Embedding using sentence transformer model\n",
    "    \n",
    "    2.4 Upsert data to pinecone\n",
    "    \n",
    "    2.5 Question & Answer Chatbot with memory\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8786a2",
   "metadata": {},
   "source": [
    "# PART 1 SCRAPPING AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c73617",
   "metadata": {},
   "source": [
    "## 1.1 Scrape data from webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0862a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f84aa3",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Generative_artificial_intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd03e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54c8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    \"\"\" \n",
    "    method to store the scraped data as a dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        data = {\n",
    "            'title': soup.title.text.strip(),\n",
    "            'paragraphs': [p.text.strip() for p in soup.find_all('p')],\n",
    "        }\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the content. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b838b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = scrape_website(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff00a59",
   "metadata": {},
   "source": [
    "## 1.2 Save the scrapped data in JSON format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37931af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(scraped_data, filename='scraped_data.json'):\n",
    "    \"\"\" \n",
    "    Save the scrapped content as a JSON file named - \"scraped_data.json\" in the working directory\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(scraped_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Data saved successfully as {filename}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d098075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully as scraped_data.json\n"
     ]
    }
   ],
   "source": [
    "save_as_json(scraped_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80a400",
   "metadata": {},
   "source": [
    "# PART 2: CHATBOT WITH MEMORY AGENT\n",
    "\n",
    "## 2.1 Retreive the JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00d0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "#import sentence tranformer for text embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#pinecone imports\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "#langchain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore,Style,Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d265c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\" \n",
    "    Remove any non ascii characters found in the text\n",
    "    \n",
    "    \"\"\"\n",
    "    return ''.join(char for char in text if ord(char) < 128)\n",
    "\n",
    "\n",
    "def process_json_data(data):\n",
    "    \"\"\" \n",
    "    Function to recursively remove non-ASCII characters from all strings in a nested data structure\n",
    "    \n",
    "    \"\"\"      \n",
    "    if isinstance(data, dict):  # If the data is a dictionary\n",
    "        return {key: process_json_data(value) for key, value in data.items()}\n",
    "    \n",
    "    elif isinstance(data, list):  # If the data is a list\n",
    "        return [process_json_data(item) for item in data]\n",
    "    \n",
    "    elif isinstance(data, str):  # If the data is a string\n",
    "        return remove_non_ascii(data)\n",
    "    \n",
    "    else:  # If the data is anything else (e.g., numbers, booleans), return it unchanged\n",
    "        return data\n",
    "\n",
    "# Load JSON data from a file\n",
    "path = 'scraped_data.json'\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the JSON data to remove non-ASCII characters\n",
    "processed_data = process_json_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d36552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content variable stroes the string of content scrapped\n",
    "content=''\n",
    "for text in processed_data['paragraphs']:\n",
    "    content+=text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b80d4f",
   "metadata": {},
   "source": [
    "## 2.2  Splitting the document to chunks (for  creating  and upserting embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c4f67",
   "metadata": {},
   "source": [
    "\n",
    " Chunk_size determined number of character in each chunk.\n",
    " \n",
    " Chunk overlap determines number of characters that has to be shared will overlapping.  \n",
    " \n",
    " Chunk size is counted as character instead of regukar expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5554066",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=20, length_function=len, is_separator_regex=False)\n",
    "\n",
    "texts = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1df00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let us try to print first 5 chunks after splitting the content to chunks\n",
      "\n",
      "Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence,\n",
      "intelligence capable of generating text, images or other data using generative models,[2] often in,\n",
      "models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and,\n",
      "the patterns and structure of their input training data and then generate new data that has similar,\n",
      "that has similar characteristics.[5][6]Improvements in transformer-based deep neural networks,\n"
     ]
    }
   ],
   "source": [
    "text_list = [] #each item in the list is a chunk\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    text_list.append(texts[i].page_content)\n",
    " \n",
    "print()\n",
    "print(\"Let us try to print first 5 chunks after splitting the content to chunks\")\n",
    "print()\n",
    "\n",
    "for item in text_list[:5]:\n",
    "    print(item,end=',\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5866ee6",
   "metadata": {},
   "source": [
    "Now, let us try to print the number of splits(chunks) for the content variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6f9333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d87310",
   "metadata": {},
   "source": [
    "## 2.3 Text Embedding using sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f75ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "#Getting the embedding list for upserting\n",
    "for i, text in enumerate(text_list, start=1):\n",
    "    embedding = model.encode(text)\n",
    "    text_dict = {\"id\": text, \"values\": np.array(embedding)}\n",
    "    embeddings_list.append(text_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09fb92",
   "metadata": {},
   "source": [
    "## 2.4 Upsert data to pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a3048",
   "metadata": {},
   "source": [
    "Upsert is a term used in pinecone vector database, similar to the term \"insert\" which writes vectors into database.\n",
    "We have to initialize the pinecone API in order to access the vector database indexes.\n",
    "\n",
    "Please Note: \n",
    "\n",
    "The free tier of pine cone only allows us to initialize the index only once. If new content needs to be upserted, then you might have to manually delete the index on the pinecone portal and create a index again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03303366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "initializing the API key - PINECONE\n",
    "\n",
    "\"\"\"\n",
    "with open('key/pinecone', 'r') as f2:\n",
    "    api_key = f2.read()\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    index_name = 'qa-bot'\n",
    "    index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58028970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name is the index being created from the pinecone dashboard\n",
    "index_name = 'qa-bot'\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e69672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 434}},\n",
       " 'total_vector_count': 434}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b93065",
   "metadata": {},
   "source": [
    "________________________________________________________________\n",
    "Next, we will try to upsert the embeddings dictionary\n",
    "\n",
    "where, \n",
    "id : text (sentences)\n",
    "value : will be the corresponding embeddings\n",
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b197b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 290}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec367d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 434}},\n",
       " 'total_vector_count': 434}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5868cc",
   "metadata": {},
   "source": [
    "## 2.5 Question & Answer Chatbot with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd14a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Read the key.txt file and set the key to the environment variable\n",
    "\n",
    "\"\"\"\n",
    "with open('key/openai', 'r') as f1:\n",
    "    openai_api_key = f1.read()\n",
    "    os.environ['OPENAI_API_KEY']=openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7815bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm =OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bb755a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_memory = ConversationBufferMemory()\n",
    "query_response_pairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f81baa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, prompt_template, query_response_pairs):\n",
    "    \"\"\" \n",
    "    Retreive documents to output the most probably answer to user\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    results = index.query(vector = [query_embedding.tolist()], top_k=5) \n",
    "\n",
    "    retrieved_documents=''\n",
    "    for result in results['matches']:\n",
    "        text = result['id']\n",
    "        retrieved_documents+=text\n",
    "\n",
    "    # Construct the prompt using the template and retrieved documents\n",
    "    prompt = prompt_template.format(documents=retrieved_documents, question=query,history = query_response_pairs)\n",
    "\n",
    "    # Generate response using the prompt\n",
    "    output_parser = StrOutputParser()\n",
    "    conversation_chain = ConversationChain(llm=llm,memory=conversation_memory,output_parser=output_parser,verbose=False)\n",
    "    response = conversation_chain.invoke(prompt)\n",
    "    query_response_pairs[query] = response['response']\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAGENT:\n",
      "\u001b[30mHey! Ask me any question you have on the web URL you upoaded for the topic -\n",
      "\u001b[1mGenerative artificial intelligence - Wikipedia\u001b[0m\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hi, what exactly is generative AI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m Generative AI, also known as GenAI or GAI, is a type of artificial intelligence that uses generative models to create new data or content. It has a wide range of uses in various industries and there are ongoing discussions about how to regulate its use. Rules are being refined to ensure responsible use of generative AI.\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how is it different from machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m Generative AI is a subset of machine learning that focuses on creating new data or content, while traditional machine learning models are trained to make predictions or classifications based on existing data. Generative AI uses unsupervised or self-supervised learning, while traditional machine learning often uses supervised learning. Additionally, generative AI models are often more complex and difficult to train compared to traditional machine learning models.\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " by the way my name is sairam. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m Nice to meet you, Sairam. Is there anything else you would like to know about generative AI?\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what was the previous questions that I was trying to ask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m  The previous questions you asked were about generative AI and how it differs from traditional machine learning. Is there anything else you would like to know?\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ok. what was my name again?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m  Your name is Sairam. Is there anything else you would like to know?\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " sorry. This is ram not sairam. what was sairam looking for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[34mAGENT     :\n",
      "\u001b[30m   My apologies, Ram. I do not have enough information to answer that question. Is there anything else you would like to know about generative AI?\n",
      "\n",
      "\n",
      "\u001b[32mUSER:\n"
     ]
    }
   ],
   "source": [
    "def output():\n",
    "    \"\"\" \n",
    "    Function to print the output of model response and \n",
    "    user interaction with the chatbot\n",
    "    \n",
    "    \"\"\"\n",
    "    bot=\"AGENT\"\n",
    "    print(Fore.BLUE + bot+\":\")\n",
    "    print(Fore.BLACK+\"Hey! Ask me any question you have on the web URL you upoaded for the topic -\")\n",
    "    print(f\"\\033[1m{data['title']}\\033[0m\")\n",
    "\n",
    "    # Run the loop for interactive conversation\n",
    "    while True:\n",
    "        print()\n",
    "        print(Fore.GREEN+\"USER:\")\n",
    "        user_input = input()\n",
    "        print()\n",
    "\n",
    "        prompt_template = \"\"\"Answer the question based in a precise way based on the following context\n",
    "        Keep the Sentence structure simple, be direct. Just give 1-3 sentences.\n",
    "        Don't make up the answer just frame the answer from the provided context \n",
    "        if it is not there say you are sorry, you don't know have enough information about this question.\n",
    "\n",
    "        Below is the context for answering the question\n",
    "        {documents}\n",
    "\n",
    "        Below is the chat history consider the below information also as context while answering the question\n",
    "        {history}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        if user_input.lower()=='stop':\n",
    "            user_input= None\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%M-%S\")\n",
    "            filename = f\"conv_{current_time}.pkl\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                pickle.dump(query_response_pairs, f)\n",
    "\n",
    "            print(Fore.BLUE +\"AGENT     :\")\n",
    "            print(Fore.BLACK+\"Thank You. Have a Nice Day\")\n",
    "\n",
    "            query_response_pairs.clear()\n",
    "            break\n",
    "\n",
    "        answer = answer_question(user_input,prompt_template,query_response_pairs)\n",
    "\n",
    "        print()\n",
    "        print(Fore.BLUE +\"AGENT     :\")\n",
    "        print(Fore.BLACK+answer['response'])\n",
    "        print()\n",
    "\n",
    "        \n",
    "output()\n",
    "\n",
    "\n",
    "#  Hi, what exactly is generative AI\n",
    "# how is it different from machine learning\n",
    "#  by the way my name is sairam. \n",
    "# what was the previous questions that I was trying to ask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffaf33",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
